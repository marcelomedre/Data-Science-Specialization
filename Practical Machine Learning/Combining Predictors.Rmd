# Combining Predictors

## Key ideas
   - combine classifiers by averaging/voting.  
   - combining classifiers imrpoves accuracy.  
   - Combining Classifiers reduces interpretability.  
   - Boosting, bagging and random forests are variantes of this theme.  
  
  
## Basic intuition - majority vote

Suppose we have 5 completely independent classifiers  

If accuracy is 70% for each:  

  - $10\times(0.7)^3(0.3)^2 + 5\times(0.7)^4(0.3)^2 + (0.7)^5$  
  - 83.7% majority vote accuracy  
  
With 101 independent classifiers  

  - 99.9% majority vote accuracy  
  
## Approaches for combining classifiers

  - Bagging, boosting, random forests  
  - Usually combine similar classifiers  
  - Combining different classifiers  
  - Model stacking  
  - Model ensembling  
  
## Example Wage data

```{r}
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage <- subset(Wage, select = -c(logwage))

# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
```

## Wage data sets

Create training, test and validation sets

```{r}
dim(training)
dim(testing)
dim(validation)
```

Build two different models

```{r}
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
              data=training, 
              trControl = trainControl(method="cv"),number=3)
```

Predict on the testing set
```{r}
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
```

Fit a model that combines predictors
```{r}
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
```

Testing errors
```{r}
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
```

Predict on validation data set
```{r}
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
```

Evaluate on validation
```{r}
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
```

Notes and further resources

  - Even simple blending can be useful  
  - Typical model for binary/multiclass data  
  - Build an odd number of models  
  - Predict with each model  
  - Predict the class by majority vote  
  - This can get dramatically more complicated  
  - Simple blending in caret: caretEnsemble (use at your own risk!)  
  - Wikipedia ensemlbe learning   