# Random Forests

## Basic Idea
   - Bootstrap samples  
   - At each split, bootstrap variables  
   - Grow multiple trees and vote
  
Pros:
  - Accuracy  
  
Cons:
  - Speed  
  - Interpretability  
  - Overfitting - important use cross validation  


## Example Iris Data   
```{r}
rm(list = ls())
data("iris")
library(ggplot2)
library(caret)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```


```{r}
modFit <- train(Species ~., data = training, method = "rf", prox = TRUE)
modFit
```

```{r}
getTree(modFit$finalModel, k = 2)
```

## Class Centers
```{r}
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP)
irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col = Species, data = training)
p + geom_point(aes(x = Petal.Width, y = Petal.Length, col = Species), size = 5, shape = 4, data = irisP)
```

## Predicting New Values
```{r}
pred <- predict(modFit, testing)
testing$predRight <- pred == testing$Species
table(pred, testing$Species)
qplot(Petal.Width, Petal.Length, colour = predRight,data = testing, main = "Newdata Predictions")
```


  - Random Forests are usually one of he top performing algrithms along with boosting in preditions contests.  
  - Random Forests are difficult to interpret but often very accurate    
  - Care should be taken to avoid overfitting (see rfcv function)  