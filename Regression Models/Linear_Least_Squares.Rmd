# Linear Least Squares

Ordinary least squares (OLS) is the workhorse of statistics. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line through some data. In the next few lectures, we cover the basics of linear least squares.

Some basic notation and background

## Notation for data

We write $X_1, X_2, \ldots, X_n$ to describe $n$ data points.
As an example, consider the data set ${1, 2, 5}$ then
$X_1 = 1$, $X_2 = 2$, $X_3 = 5$ and $n = 3$.
We often use a different letter than $X$, such as $Y_1, \ldots , Y_n$.
We will typically use Greek letters for things we don't know. Such as, $\mu$ is a mean that we'd like to estimate.

## The empirical mean

Define the empirical mean as $$ \bar X = \frac{1}{n}\sum_{i=1}^n X_i. $$
Notice if we subtract the mean from data points, we get data that has mean 0. That is, if we define $$ \tilde X_i = X_i - \bar X. $$ The mean of the $\tilde X_i$ is 0.
This process is called "centering" the random variables.
Recall from the previous lecture that the mean is the least squares solution for minimizing $$ \sum_{i=1}^n (X_i - \mu)^2 $$

## The emprical standard deviation and variance

Define the empirical variance as $$ S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2 = \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \bar X ^ 2 \right) $$
The empirical standard deviation is defined as $S = \sqrt{S^2}$. Notice that the standard deviation has the same units as the data.
The data defined by $X_i / s$ have empirical standard deviation 1. This is called "scaling" the data.

## Normalization

The data defined by $$ Z_i = \frac{X_i - \bar X}{s} $$ have empirical mean zero and empirical standard deviation 1.
The process of centering then scaling the data is called "normalizing" the data.
Normalized data are centered at 0 and have units equal to standard deviations of the original data.
Example, a value of 2 from normalized data means that data point was two standard deviations larger than the mean.

## The empirical covariance

Consider now when we have pairs of data, $(X_i, Y_i)$.
Their empirical covariance is $$ Cov(X, Y) = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X) (Y_i - \bar Y) = \frac{1}{n-1}\left( \sum_{i=1}^n X_i Y_i - n \bar X \bar Y\right) $$
The correlation is defined is $$ Cor(X, Y) = \frac{Cov(X, Y)}{S_x S_y} $$ where $S_x$ and $S_y$ are the estimates of standard deviations for the $X$ observations and $Y$ observations, respectively.

## Some facts about correlation

$Cor(X, Y) = Cor(Y, X)$
$-1 \leq Cor(X, Y) \leq 1$
$Cor(X,Y) = 1$ and $Cor(X, Y) = -1$ only when the $X$ or $Y$ observations fall perfectly on a positive or negative sloped line, respectively.
$Cor(X, Y)$ measures the strength of the linear relationship between the $X$ and $Y$ data, with stronger relationships as $Cor(X,Y)$ heads towards -1 or 1.
$Cor(X, Y) = 0$ implies no linear relationship.

# make this an external chunk that can be included in any file

options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
runif(1)
```

## General least squares for linear equations
Consider again the parent and child height data from Galton

```{r, fig.height=5, fig.width=8, echo=FALSE}
library(UsingR)
data(galton)
library(dplyr); library(ggplot2)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")  
g
```

---
## Fitting the best line
* Let $Y_i$ be the $i^{th}$ child's height and $X_i$ be the 
$i^{th}$ (average over the pair of) parents' heights. 
* Consider finding the best line 
  * Child's Height = $\beta_0$ + Parent's Height $\beta_1$
* Use least squares
  $$
  \sum_{i=1}^n \{Y_i - (\beta_0 + \beta_1 X_i)\}^2
  $$

---
## Results
* The least squares model fit to the line $Y = \beta_0 + \beta_1 X$ through the data pairs $(X_i, Y_i)$ with $Y_i$ as the outcome obtains the line $Y = \hat \beta_0 + \hat \beta_1 X$ where
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$
* $\hat \beta_1$ has the units of $Y / X$, $\hat \beta_0$ has the units of $Y$.
* The line passes through the point $(\bar X, \bar Y$)
* The slope of the regression line with $X$ as the outcome and $Y$ as the predictor is $Cor(Y, X) Sd(X)/ Sd(Y)$. 
* The slope is the same one you would get if you centered the data,
$(X_i - \bar X, Y_i - \bar Y)$, and did regression through the origin.
* If you normalized the data, $\{ \frac{X_i - \bar X}{Sd(X)}, \frac{Y_i - \bar Y}{Sd(Y)}\}$, the slope is $Cor(Y, X)$.


---
## Revisiting Galton's data
### Double check our calculations using R
```{r, fig.height=4,fig.width=4,echo=TRUE}
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
```

---
## Revisiting Galton's data
### Reversing the outcome/predictor relationship
```{r, fig.height=4,fig.width=4,echo=TRUE}
beta1 <- cor(y, x) *  sd(x) / sd(y)
beta0 <- mean(x) - beta1 * mean(y)
rbind(c(beta0, beta1), coef(lm(x ~ y)))
```

---
## Revisiting Galton's data
### Regression through the origin yields an equivalent slope if you center the data first
```{r, fig.height=4,fig.width=4,echo=TRUE}
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])
```

---
## Revisiting Galton's data
### Normalizing variables results in the slope being the correlation
```{r, echo=TRUE}
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

---
```{r, fig.height=6,fig.width=6,echo=FALSE}
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")  
g <- g + geom_smooth(method="lm", formula=y~x)
g
```